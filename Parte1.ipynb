{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fcdd3d91-55b0-4025-b353-633efd2e4d48",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557428f6-bceb-4c3f-b092-22440e9e46dd",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Cargue el conjunto de datos de Iris utilizando bibliotecas como sklearn.datasets. Luego, divida el conjunto de datos  en conjuntos de entrenamiento y validación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19f124e4-f79b-4aec-9af4-cf2d1f778752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del set de entrenamiento: 105\n",
      "Tamaño del set de validación: 45\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data         \n",
    "y = iris.target    \n",
    "\n",
    "# Normalizar características\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(\"Tamaño del set de entrenamiento:\", len(train_dataset))\n",
    "print(\"Tamaño del set de validación:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456bb33d-3f9c-4dce-aa67-ec39e86c8c3a",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Cree una red neuronal feedforward simple utilizando nn.Module de PyTorch. Luego, defina capa de entrada, capas  ocultas y capa de salida. Después, elija las funciones de activación y el número de neuronas por capa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a4ab92f-3cb5-48de-9062-53706591238a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedforwardNN(\n",
      "  (fc1): Linear(in_features=4, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=8, out_features=3, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_dim=4, hidden1=16, hidden2=8, output_dim=3):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))   \n",
    "        x = self.relu(self.fc2(x))  \n",
    "        x = self.fc3(x)            \n",
    "        return x\n",
    "\n",
    "model = FeedforwardNN()\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc74c5c3-0c3c-4961-8ecd-31f8caa308dd",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Utilice diferentes funciones de pérdida comunes como Cross-Entropy Loss y MSE para clasificación. Entrene el  modelo con diferentes funciones de pérdida y registre las pérdidas de entrenamiento y test. Debe utilizar al menos 3  diferentes funciones. Es decir, procure que su código sea capaz de parametrizar el uso de diferentes funciones de  pérdida.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edf2ebdc-ebfb-4e84-83be-557f68702cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando con CROSS_ENTROPY...\n",
      "\n",
      "[cross_entropy] Epoch 10/50 Train Loss: 0.0966, Val Loss: 0.1570\n",
      "[cross_entropy] Epoch 20/50 Train Loss: 0.0473, Val Loss: 0.1562\n",
      "[cross_entropy] Epoch 30/50 Train Loss: 0.0266, Val Loss: 0.1373\n",
      "[cross_entropy] Epoch 40/50 Train Loss: 0.0280, Val Loss: 0.1639\n",
      "[cross_entropy] Epoch 50/50 Train Loss: 0.0173, Val Loss: 0.1479\n",
      "\n",
      "Entrenando con MSE...\n",
      "\n",
      "[mse] Epoch 10/50 Train Loss: 0.0319, Val Loss: 0.0436\n",
      "[mse] Epoch 20/50 Train Loss: 0.0130, Val Loss: 0.0245\n",
      "[mse] Epoch 30/50 Train Loss: 0.0101, Val Loss: 0.0242\n",
      "[mse] Epoch 40/50 Train Loss: 0.0121, Val Loss: 0.0247\n",
      "[mse] Epoch 50/50 Train Loss: 0.0088, Val Loss: 0.0271\n",
      "\n",
      "Entrenando con NLL...\n",
      "\n",
      "[nll] Epoch 10/50 Train Loss: 0.1083, Val Loss: 0.1907\n",
      "[nll] Epoch 20/50 Train Loss: 0.0356, Val Loss: 0.1303\n",
      "[nll] Epoch 30/50 Train Loss: 0.0409, Val Loss: 0.1711\n",
      "[nll] Epoch 40/50 Train Loss: 0.0188, Val Loss: 0.1585\n",
      "[nll] Epoch 50/50 Train Loss: 0.0133, Val Loss: 0.1754\n"
     ]
    }
   ],
   "source": [
    "def train_model(loss_fn_name, train_loader, val_loader, epochs=50, lr=0.01):\n",
    "    input_dim, output_dim = 4, 3\n",
    "    model = FeedforwardNN(input_dim=input_dim, output_dim=output_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    loss_functions = {\n",
    "        \"cross_entropy\": nn.CrossEntropyLoss(),\n",
    "        \"mse\": nn.MSELoss(),\n",
    "        \"nll\": nn.NLLLoss()\n",
    "    }\n",
    "    criterion = loss_functions[loss_fn_name]\n",
    "\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "\n",
    "            if loss_fn_name == \"mse\":\n",
    "                y_batch_oh = nn.functional.one_hot(y_batch, num_classes=output_dim).float()\n",
    "                loss = criterion(outputs, y_batch_oh)\n",
    "            elif loss_fn_name == \"nll\":\n",
    "                log_probs = nn.functional.log_softmax(outputs, dim=1)\n",
    "                loss = criterion(log_probs, y_batch)\n",
    "            else:\n",
    "                loss = criterion(outputs, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                outputs = model(X_val)\n",
    "\n",
    "                if loss_fn_name == \"mse\":\n",
    "                    y_val_oh = nn.functional.one_hot(y_val, num_classes=output_dim).float()\n",
    "                    loss = criterion(outputs, y_val_oh)\n",
    "                elif loss_fn_name == \"nll\":\n",
    "                    log_probs = nn.functional.log_softmax(outputs, dim=1)\n",
    "                    loss = criterion(log_probs, y_val)\n",
    "                else:\n",
    "                    loss = criterion(outputs, y_val)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss / len(train_loader))\n",
    "        history[\"val_loss\"].append(val_loss / len(val_loader))\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"[{loss_fn_name}] Epoch {epoch+1}/{epochs} \"\n",
    "                  f\"Train Loss: {history['train_loss'][-1]:.4f}, \"\n",
    "                  f\"Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "losses = {}\n",
    "for loss_name in [\"cross_entropy\", \"mse\", \"nll\"]:\n",
    "    print(f\"\\nEntrenando con {loss_name.upper()}...\\n\")\n",
    "    history = train_model(loss_name, train_loader, val_loader, epochs=50, lr=0.01)\n",
    "    losses[loss_name] = history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344c5c34-840e-4a33-b531-b302f4c7029f",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Utilice distintas técnicas de regularización como L1, L2 y dropout. Entrene el modelo con y sin técnicas de  regularización y observe el impacto en el overfitting y la generalización. Debe utilizar al menos 3 diferentes técnicas.  Es decir, procure que su código sea capaz de parametrizar el uso de diferentes técnicas de regularización. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "001a8c20-2f22-4a38-bf5d-2799cc11d3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sin regularización (CrossEntropy) ===\n",
      "[cross_entropy] Epoch 1/50 Train Loss: 0.9925, Val Loss: 0.8452\n",
      "[cross_entropy] Epoch 11/50 Train Loss: 0.0711, Val Loss: 0.1258\n",
      "[cross_entropy] Epoch 21/50 Train Loss: 0.0382, Val Loss: 0.1297\n",
      "[cross_entropy] Epoch 31/50 Train Loss: 0.0295, Val Loss: 0.1852\n",
      "[cross_entropy] Epoch 41/50 Train Loss: 0.0205, Val Loss: 0.1571\n",
      "\n",
      "=== Regularización L2 ===\n",
      "[cross_entropy] Epoch 1/50 Train Loss: 1.0768, Val Loss: 0.9813\n",
      "[cross_entropy] Epoch 11/50 Train Loss: 0.0706, Val Loss: 0.1257\n",
      "[cross_entropy] Epoch 21/50 Train Loss: 0.0622, Val Loss: 0.1271\n",
      "[cross_entropy] Epoch 31/50 Train Loss: 0.0505, Val Loss: 0.1347\n",
      "[cross_entropy] Epoch 41/50 Train Loss: 0.0518, Val Loss: 0.1156\n",
      "\n",
      "=== Regularización L1 ===\n",
      "[cross_entropy] Epoch 1/50 Train Loss: 1.4000, Val Loss: 1.0108\n",
      "[cross_entropy] Epoch 11/50 Train Loss: 0.6930, Val Loss: 0.5126\n",
      "[cross_entropy] Epoch 21/50 Train Loss: 0.4939, Val Loss: 0.2841\n",
      "[cross_entropy] Epoch 31/50 Train Loss: 0.4338, Val Loss: 0.2188\n",
      "[cross_entropy] Epoch 41/50 Train Loss: 0.3968, Val Loss: 0.1906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1.3999747037887573,\n",
       "  1.235278878893171,\n",
       "  1.124216045652117,\n",
       "  1.058259972504207,\n",
       "  0.9939755541937692,\n",
       "  0.9243466428347996,\n",
       "  0.8819808959960938,\n",
       "  0.8255657127925328,\n",
       "  0.7880617039544242,\n",
       "  0.7312228083610535,\n",
       "  0.6929844532694135,\n",
       "  0.6571718880108425,\n",
       "  0.6281284434454781,\n",
       "  0.598013094493321,\n",
       "  0.567102347101484,\n",
       "  0.563959002494812,\n",
       "  0.5334761696202415,\n",
       "  0.5342325781072889,\n",
       "  0.5063458936555045,\n",
       "  0.5065822431019374,\n",
       "  0.4939098613602774,\n",
       "  0.4877473669392722,\n",
       "  0.4684529347079141,\n",
       "  0.4767651515347617,\n",
       "  0.46279240931783405,\n",
       "  0.46076370988573345,\n",
       "  0.44302694712366375,\n",
       "  0.44098777430398123,\n",
       "  0.4395856431552342,\n",
       "  0.4295825575079237,\n",
       "  0.43375788416181293,\n",
       "  0.42276023966925486,\n",
       "  0.4328600083078657,\n",
       "  0.41850124086652485,\n",
       "  0.41168512191091267,\n",
       "  0.4090933586869921,\n",
       "  0.40827155113220215,\n",
       "  0.4093503398554666,\n",
       "  0.40553433128765654,\n",
       "  0.4055331902844565,\n",
       "  0.3967679100377219,\n",
       "  0.39293050340243746,\n",
       "  0.38991556423051016,\n",
       "  0.40320644208363127,\n",
       "  0.3971408222402845,\n",
       "  0.389890764440809,\n",
       "  0.393095110143934,\n",
       "  0.39175104243414743,\n",
       "  0.3840845652988979,\n",
       "  0.3800981172493526],\n",
       " [1.0108156204223633,\n",
       "  0.9452853202819824,\n",
       "  0.8909463286399841,\n",
       "  0.8366338213284811,\n",
       "  0.7837998270988464,\n",
       "  0.7355241179466248,\n",
       "  0.6892860531806946,\n",
       "  0.6428112188975016,\n",
       "  0.5953261256217957,\n",
       "  0.5539180040359497,\n",
       "  0.5125568409760793,\n",
       "  0.4744192560513814,\n",
       "  0.43936794996261597,\n",
       "  0.4134713113307953,\n",
       "  0.3795684377352397,\n",
       "  0.3553840716679891,\n",
       "  0.35549108187357586,\n",
       "  0.32141731182734173,\n",
       "  0.31525225440661114,\n",
       "  0.29123955965042114,\n",
       "  0.28412312269210815,\n",
       "  0.27264414727687836,\n",
       "  0.2604152361551921,\n",
       "  0.2529567430416743,\n",
       "  0.23970724642276764,\n",
       "  0.2561732431252797,\n",
       "  0.22693331042925516,\n",
       "  0.22775217394034067,\n",
       "  0.2243811090787252,\n",
       "  0.21072789033253989,\n",
       "  0.21877850592136383,\n",
       "  0.20602871477603912,\n",
       "  0.2029027392466863,\n",
       "  0.22962127129236856,\n",
       "  0.20672709743181863,\n",
       "  0.2050927678743998,\n",
       "  0.2053348422050476,\n",
       "  0.20591992139816284,\n",
       "  0.19713397324085236,\n",
       "  0.21141397456328073,\n",
       "  0.19064292311668396,\n",
       "  0.1979831407467524,\n",
       "  0.2012456307808558,\n",
       "  0.18931918839613596,\n",
       "  0.19957260290781656,\n",
       "  0.19028464456399283,\n",
       "  0.19153477251529694,\n",
       "  0.2094251662492752,\n",
       "  0.18509817123413086,\n",
       "  0.19312323133150736])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_model_regularization(loss_fn_name=\"cross_entropy\", train_loader=None, val_loader=None,\n",
    "                               regularization=None, reg_lambda=0.01, epochs=100):\n",
    "    \n",
    "    model = FeedforwardNN(input_dim=4, output_dim=3)\n",
    "    \n",
    "    if loss_fn_name == \"cross_entropy\":\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif loss_fn_name == \"mse\":\n",
    "        criterion = nn.MSELoss()\n",
    "    elif loss_fn_name == \"nll\":\n",
    "        criterion = nn.NLLLoss()\n",
    "    else:\n",
    "        raise ValueError(\"Función de pérdida no soportada.\")\n",
    "    \n",
    "    # Optimizer con L2 si se selecciona\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01,\n",
    "                           weight_decay=reg_lambda if regularization==\"l2\" else 0.0)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        batch_train_loss = 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "\n",
    "            if loss_fn_name == \"mse\":\n",
    "                y_batch_oh = nn.functional.one_hot(y_batch, num_classes=3).float()\n",
    "                loss = criterion(outputs, y_batch_oh)\n",
    "            elif loss_fn_name == \"nll\":\n",
    "                log_probs = nn.functional.log_softmax(outputs, dim=1)\n",
    "                loss = criterion(log_probs, y_batch)\n",
    "            else:\n",
    "                loss = criterion(outputs, y_batch)\n",
    "\n",
    "            if regularization == \"l1\":\n",
    "                l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "                loss = loss + reg_lambda * l1_norm\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_train_loss += loss.item()\n",
    "\n",
    "        train_losses.append(batch_train_loss / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        batch_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                outputs = model(X_batch)\n",
    "                if loss_fn_name == \"mse\":\n",
    "                    y_batch_oh = nn.functional.one_hot(y_batch, num_classes=3).float()\n",
    "                    loss = criterion(outputs, y_batch_oh)\n",
    "                elif loss_fn_name == \"nll\":\n",
    "                    log_probs = nn.functional.log_softmax(outputs, dim=1)\n",
    "                    loss = criterion(log_probs, y_batch)\n",
    "                else:\n",
    "                    loss = criterion(outputs, y_batch)\n",
    "                batch_val_loss += loss.item()\n",
    "\n",
    "        val_losses.append(batch_val_loss / len(val_loader))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"[{loss_fn_name}] Epoch {epoch+1}/{epochs} \"\n",
    "                  f\"Train Loss: {train_losses[-1]:.4f}, \"\n",
    "                  f\"Val Loss: {val_losses[-1]:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "print(\"\\n=== Sin regularización (CrossEntropy) ===\")\n",
    "train_model_regularization(loss_fn_name=\"cross_entropy\",\n",
    "                           train_loader=train_loader,\n",
    "                           val_loader=val_loader,\n",
    "                           regularization=None,\n",
    "                           epochs=50)\n",
    "\n",
    "print(\"\\n=== Regularización L2 ===\")\n",
    "train_model_regularization(loss_fn_name=\"cross_entropy\",\n",
    "                           train_loader=train_loader,\n",
    "                           val_loader=val_loader,\n",
    "                           regularization=\"l2\",\n",
    "                           reg_lambda=0.01,\n",
    "                           epochs=50)\n",
    "\n",
    "print(\"\\n=== Regularización L1 ===\")\n",
    "train_model_regularization(loss_fn_name=\"cross_entropy\",\n",
    "                           train_loader=train_loader,\n",
    "                           val_loader=val_loader,\n",
    "                           regularization=\"l1\",\n",
    "                           reg_lambda=0.01,\n",
    "                           epochs=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915f449a-cfc0-4422-b5f4-30646a39bbd3",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "Utilice distintas técnicas de optimización como SGD, Batch GD, Mini-Batch GD. Entrene el modelo con algoritmos de  optimización y registre las pérdidas y tiempos de entrenamiento y test. Debe utilizar al menos 3 diferentes algoritmos.  Es decir, procure que su código sea capaz de parametrizar el uso de diferentes algoritmos de optimización.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76bdca32-8a88-4972-a96b-7ff8d5715e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SGD | cross_entropy] Epoch 10/50 Train Loss: 1.1176, Val Loss: 1.1099\n",
      "[SGD | cross_entropy] Epoch 20/50 Train Loss: 1.1093, Val Loss: 1.1042\n",
      "[SGD | cross_entropy] Epoch 30/50 Train Loss: 1.1019, Val Loss: 1.0990\n",
      "[SGD | cross_entropy] Epoch 40/50 Train Loss: 1.0958, Val Loss: 1.0942\n",
      "[SGD | cross_entropy] Epoch 50/50 Train Loss: 1.0905, Val Loss: 1.0897\n",
      "Tiempo total de entrenamiento con SGD: 0.12 s\n",
      "[SGD | cross_entropy] Epoch 10/50 Train Loss: 1.1105, Val Loss: 1.1125\n",
      "[SGD | cross_entropy] Epoch 20/50 Train Loss: 1.0844, Val Loss: 1.0909\n",
      "[SGD | cross_entropy] Epoch 30/50 Train Loss: 1.0623, Val Loss: 1.0719\n",
      "[SGD | cross_entropy] Epoch 40/50 Train Loss: 1.0327, Val Loss: 1.0470\n",
      "[SGD | cross_entropy] Epoch 50/50 Train Loss: 0.9915, Val Loss: 1.0090\n",
      "Tiempo total de entrenamiento con SGD: 0.22 s\n",
      "[SGD | cross_entropy] Epoch 10/50 Train Loss: 0.2232, Val Loss: 0.2879\n",
      "[SGD | cross_entropy] Epoch 20/50 Train Loss: 0.0807, Val Loss: 0.1352\n",
      "[SGD | cross_entropy] Epoch 30/50 Train Loss: 0.0498, Val Loss: 0.0990\n",
      "[SGD | cross_entropy] Epoch 40/50 Train Loss: 0.0446, Val Loss: 0.0928\n",
      "[SGD | cross_entropy] Epoch 50/50 Train Loss: 0.0430, Val Loss: 0.1234\n",
      "Tiempo total de entrenamiento con SGD: 2.24 s\n",
      "[ADAM | cross_entropy] Epoch 10/50 Train Loss: 0.0893, Val Loss: 0.1621\n",
      "[ADAM | cross_entropy] Epoch 20/50 Train Loss: 0.0312, Val Loss: 0.1466\n",
      "[ADAM | cross_entropy] Epoch 30/50 Train Loss: 0.0246, Val Loss: 0.3332\n",
      "[ADAM | cross_entropy] Epoch 40/50 Train Loss: 0.0210, Val Loss: 0.2095\n",
      "[ADAM | cross_entropy] Epoch 50/50 Train Loss: 0.0134, Val Loss: 0.2519\n",
      "Tiempo total de entrenamiento con ADAM: 0.31 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': [0.9508827073233468,\n",
       "  0.7249012291431427,\n",
       "  0.5459460318088531,\n",
       "  0.3934256434440613,\n",
       "  0.2929522288697107,\n",
       "  0.24170797318220139,\n",
       "  0.19136374443769455,\n",
       "  0.14217776459242618,\n",
       "  0.1121454547558512,\n",
       "  0.0893327673631055,\n",
       "  0.06934472785464355,\n",
       "  0.06323612574487925,\n",
       "  0.05478259269148111,\n",
       "  0.0541815043294004,\n",
       "  0.0620681940178786,\n",
       "  0.04266122356057167,\n",
       "  0.04392902000940272,\n",
       "  0.03602570425053792,\n",
       "  0.03725146608693259,\n",
       "  0.031241179709987982,\n",
       "  0.02933008755956377,\n",
       "  0.028903043030628135,\n",
       "  0.029867252966921245,\n",
       "  0.028012561611831188,\n",
       "  0.035105986562224904,\n",
       "  0.029263572334977134,\n",
       "  0.032151972908260565,\n",
       "  0.05637143825047782,\n",
       "  0.06631934948797737,\n",
       "  0.02457059665383505,\n",
       "  0.041283142154238055,\n",
       "  0.04073597191849591,\n",
       "  0.03444064791048212,\n",
       "  0.02759074528668342,\n",
       "  0.021939921392393962,\n",
       "  0.02310839497866774,\n",
       "  0.020015340564506396,\n",
       "  0.020398680933534967,\n",
       "  0.01887124597227999,\n",
       "  0.021005026547105184,\n",
       "  0.018254985168044056,\n",
       "  0.015524186509927469,\n",
       "  0.01657999990441437,\n",
       "  0.014564903178584896,\n",
       "  0.01612931924007301,\n",
       "  0.014235077899814184,\n",
       "  0.014613023393654398,\n",
       "  0.014105059104622342,\n",
       "  0.016415159941451356,\n",
       "  0.013414514488041667],\n",
       " 'val_loss': [0.8749692440032959,\n",
       "  0.6802443464597067,\n",
       "  0.5353311796983083,\n",
       "  0.42901336153348285,\n",
       "  0.370976855357488,\n",
       "  0.32077784339586896,\n",
       "  0.2719815969467163,\n",
       "  0.22890244921048483,\n",
       "  0.1934904158115387,\n",
       "  0.16213217129309973,\n",
       "  0.14132637530565262,\n",
       "  0.13479806731144586,\n",
       "  0.1308919166525205,\n",
       "  0.12705803910891214,\n",
       "  0.1278257668018341,\n",
       "  0.14851151903470358,\n",
       "  0.13732018818457922,\n",
       "  0.14107120037078857,\n",
       "  0.15813783059517542,\n",
       "  0.14663851509491602,\n",
       "  0.15638314187526703,\n",
       "  0.1595125993092855,\n",
       "  0.15624783436457315,\n",
       "  0.18154860039552054,\n",
       "  0.1509830206632614,\n",
       "  0.16709191600481668,\n",
       "  0.17990303287903467,\n",
       "  0.20756558577219644,\n",
       "  0.14935683024426302,\n",
       "  0.33321935931841534,\n",
       "  0.17576465010643005,\n",
       "  0.18942910929520926,\n",
       "  0.22918547689914703,\n",
       "  0.1617609883348147,\n",
       "  0.22829840580622354,\n",
       "  0.1987168292204539,\n",
       "  0.17798114567995071,\n",
       "  0.21895003815491995,\n",
       "  0.21013376116752625,\n",
       "  0.20945735772450766,\n",
       "  0.19140262653430304,\n",
       "  0.23217550913492838,\n",
       "  0.20711302508910498,\n",
       "  0.2094958871603012,\n",
       "  0.21542315185070038,\n",
       "  0.2167077288031578,\n",
       "  0.2290561000506083,\n",
       "  0.2147314896186193,\n",
       "  0.2375484456618627,\n",
       "  0.2518836309512456],\n",
       " 'train_time': 0.31374573707580566}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_model_optim(loss_fn_name=\"cross_entropy\", train_loader=None, val_loader=None,\n",
    "                      optimizer_name=\"adam\", batch_size=16, epochs=50, lr=0.01):\n",
    "    \n",
    "    input_dim, output_dim = 4, 3\n",
    "    model = FeedforwardNN(input_dim=input_dim, output_dim=output_dim)\n",
    "    \n",
    "    loss_functions = {\n",
    "        \"cross_entropy\": nn.CrossEntropyLoss(),\n",
    "        \"mse\": nn.MSELoss(),\n",
    "        \"nll\": nn.NLLLoss()\n",
    "    }\n",
    "    criterion = loss_functions[loss_fn_name]\n",
    "\n",
    "    if optimizer_name.lower() == \"adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name.lower() == \"sgd\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise ValueError(\"Optimización no soportada\")\n",
    "\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"train_time\": 0.0}\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "\n",
    "            if loss_fn_name == \"mse\":\n",
    "                y_batch_oh = nn.functional.one_hot(y_batch, num_classes=output_dim).float()\n",
    "                loss = criterion(outputs, y_batch_oh)\n",
    "            elif loss_fn_name == \"nll\":\n",
    "                log_probs = nn.functional.log_softmax(outputs, dim=1)\n",
    "                loss = criterion(log_probs, y_batch)\n",
    "            else:\n",
    "                loss = criterion(outputs, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                outputs = model(X_val)\n",
    "                if loss_fn_name == \"mse\":\n",
    "                    y_val_oh = nn.functional.one_hot(y_val, num_classes=output_dim).float()\n",
    "                    loss = criterion(outputs, y_val_oh)\n",
    "                elif loss_fn_name == \"nll\":\n",
    "                    log_probs = nn.functional.log_softmax(outputs, dim=1)\n",
    "                    loss = criterion(log_probs, y_val)\n",
    "                else:\n",
    "                    loss = criterion(outputs, y_val)\n",
    "                val_loss += loss.item()\n",
    "        history[\"val_loss\"].append(val_loss / len(val_loader))\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"[{optimizer_name.upper()} | {loss_fn_name}] Epoch {epoch+1}/{epochs} \"\n",
    "                  f\"Train Loss: {history['train_loss'][-1]:.4f}, \"\n",
    "                  f\"Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "    history[\"train_time\"] = time.time() - start_time\n",
    "    print(f\"Tiempo total de entrenamiento con {optimizer_name.upper()}: {history['train_time']:.2f} s\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "batch_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "train_model_optim(loss_fn_name=\"cross_entropy\", train_loader=batch_loader,\n",
    "                  val_loader=val_loader, optimizer_name=\"sgd\", batch_size=len(train_dataset))\n",
    "\n",
    "train_model_optim(loss_fn_name=\"cross_entropy\", train_loader=train_loader,\n",
    "                  val_loader=val_loader, optimizer_name=\"sgd\", batch_size=16)\n",
    "\n",
    "sgd_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "train_model_optim(loss_fn_name=\"cross_entropy\", train_loader=sgd_loader,\n",
    "                  val_loader=val_loader, optimizer_name=\"sgd\", batch_size=1)\n",
    "\n",
    "train_model_optim(loss_fn_name=\"cross_entropy\", train_loader=train_loader,\n",
    "                  val_loader=val_loader, optimizer_name=\"adam\", batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d57e969-1a2b-4c74-8dfd-b3b2c2718b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
